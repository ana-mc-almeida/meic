{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Klu6TkY_YG7k"
   },
   "source": [
    "In this lab we will study the initial approaches in the context of multiagent systems based on game theory. Mainly the concept of Nash equilibria including repeated games.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3ucKq9hYf5p"
   },
   "source": [
    "# Installation\n",
    "For this lab we will only need numpy. The toolbox nashpy has already several algorithms implemented and allows to verify some of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5JSUVCLdpnW"
   },
   "outputs": [],
   "source": [
    "#nashpy toolbox\n",
    "!pip install nashpy\n",
    "!pip install matplotlib\n",
    "#Knight, V., & Campbell, J. (2018). Nashpy: A Python library for the computation of Nash equilibria. Journal of Open Source Software, 3(30), 904.\n",
    "#https://nashpy.readthedocs.io/en/stable/text-book/index.html\n",
    "#https://nashpy.readthedocs.io/en/stable/\n",
    "\n",
    "import nashpy as nash\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HI6nIOytezx4"
   },
   "outputs": [],
   "source": [
    "# @title Define Games\n",
    "#\n",
    "# We can use the toolbox to define some standard games.\n",
    "import warnings\n",
    "\n",
    "Games = {}\n",
    "A = np.array([[1, -1], [-1, 1]])\n",
    "Games['Matching_pennies'] = nash.Game(A)\n",
    "\n",
    "A = np.array([[-1, 1], [1, -1]])\n",
    "Games['UnMatching_pennies'] = nash.Game(A)\n",
    "\n",
    "A = np.array([[1, 3], [0, 2]])\n",
    "B = np.array([[1, 0], [3, 2]])\n",
    "Games['Prisioner_dilema'] = nash.Game(A,B)\n",
    "\n",
    "A = np.array([[-2, -2], [6, 0]])\n",
    "B = np.array([[0, 6], [3, 3]])\n",
    "Games['Hawk-Dove'] = nash.Game(A,B)\n",
    "\n",
    "A = np.array([[0,-1,1],[1,0,-1],[-1,1,0]])\n",
    "Games['Rock-Paper-Scissor'] = nash.Game(A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LFunBY9CjpI"
   },
   "source": [
    "# Activity 1 - Nash equilibria\n",
    "For each of the previous games verify if a Nash equilibria exists in pure strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qD3wbZ2f14nk"
   },
   "outputs": [],
   "source": [
    "#Compute pure Nash equilibria\n",
    "def findPureNash(g):\n",
    "    nasheq = []\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "    return nasheq\n",
    "\n",
    "for kk in Games.keys():\n",
    "    g = Games[kk]\n",
    "    ret = findPureNash(g)\n",
    "    if ret:\n",
    "        print(kk, \"Pure nash equilibria  \", ret)\n",
    "    else:\n",
    "        print(kk, \"has no pure NE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9N5vcleDCXl"
   },
   "source": [
    "You can verify your solution with the following code, that uses the various methods of the toolbox, to compute the Nash equilibria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7BjnLJ6uDvl"
   },
   "outputs": [],
   "source": [
    "# game solutions based on Nashpy\n",
    "\n",
    "for kk in Games.keys():\n",
    "    g = Games[kk]\n",
    "\n",
    "    if g.zero_sum:\n",
    "        a = g.linear_program()\n",
    "    else:\n",
    "        a = g.vertex_enumeration()\n",
    "        #b = g.support_enumeration()\n",
    "    print(kk,\"\\n Nash equilibria\\n\",list(a))\n",
    "\n",
    "    # Ficticious Play\n",
    "    np.random.seed(0)\n",
    "    iterations = 5000\n",
    "    play_counts = g.fictitious_play(iterations=iterations)\n",
    "    play_counts=np.array(list(play_counts))\n",
    "    print(\" ficticious play\\n\", play_counts[-1,:,:]/(play_counts.shape[0]-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LrQhTXy2CKG"
   },
   "source": [
    "# Activity 2\n",
    "Nash equilibria is in many cases not the best for both players and other strategies have been proposed. Define 3 strategies, always Nash, Tit-for-Tat (repeat the action of the other player), or Tit-for-Two-Tats (defect only after two defects). Make games between all the types of agents. What is the best strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJDfUwjX2ACa"
   },
   "outputs": [],
   "source": [
    "strat = ['nash','t4t','t42t']\n",
    "N = 100\n",
    "g = Games['Prisioner_dilema']\n",
    "\n",
    "# player - player number 0/1\n",
    "# P - list with the previous plays\n",
    "# strat - strategy of the player ['nash','t4t','t42t']\n",
    "# it should return a pure strategy as a vector, e.g. [1,0] to choose the first action\n",
    "def play( player, P, strat):\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "    return st\n",
    "\n",
    "\n",
    "for strat0 in strat:\n",
    "    for strat1 in strat:\n",
    "        P = [[np.random.randint(0,2),np.random.randint(0,2)]]\n",
    "        R = []\n",
    "        for nn in range(0,N):\n",
    "            p0 = play( 0, P, strat0)\n",
    "            p1 = play( 1, P, strat1)\n",
    "            P.append([p0.index(1),p1.index(1)])\n",
    "            r = g[p0,p1]\n",
    "            R.append([r[0],r[1]])\n",
    "        print(strat0,strat1,np.sum(np.array(R),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MquruzRNBa99"
   },
   "source": [
    "# Questions\n",
    "Is playing the NE always better?\n",
    "\n",
    "Are these strategies meaningfull in all games? How do the different strategies result in the different games?\n",
    "\n",
    "Which of the previous strategies is more robust to the shaking hand problem, i.e. when an agent wants to make an action but - by mistake - does another? How can you change the code to verify it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNjfq9cWD1tb"
   },
   "source": [
    "# Activity 3\n",
    "Similar to game theory, Evolutionary stable strategies explain how the strategy of a complete species can evolve. The `g.replicator_dynamics` function for the toolbox can compute this as in this code.\n",
    "Try to replicate the code in the following cell.\n",
    "Answer the following questions. Which games have an ESS? How does it related with the existing Nash Equilibria? What are the properties of the games where there is not an ESS?\n",
    "\n",
    "[Evolutionary stable strategies](https://nashpy.readthedocs.io/en/v0.0.22/reference/replicator-dynamics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-_f0uEXd33z"
   },
   "outputs": [],
   "source": [
    "# @title ESS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g = Games['Rock-Paper-Scissor']\n",
    "y0 = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "nA = g.payoff_matrices[0].shape[0]\n",
    "\n",
    "mut = 0.0\n",
    "mutation = np.eye(nA)*(1-mut)+(1-np.eye(nA))*mut/(nA-1)\n",
    "\n",
    "play_counts = g.replicator_dynamics(y0=y0, timepoints=np.linspace(0, 10, 100)\n",
    "                                        ,mutation_matrix=mutation)\n",
    "print(play_counts[-1,:])\n",
    "plt.plot(play_counts)\n",
    "plt.legend(['Rock','Paper','Scissor'])\n",
    "\n",
    "\n",
    "g = Games['Prisioner_dilema']\n",
    "y0 = np.array([0.1,0.9])\n",
    "\n",
    "nA = g.payoff_matrices[0].shape[0]\n",
    "\n",
    "mut = 0.0\n",
    "mutation = np.eye(nA)*(1-mut)+(1-np.eye(nA))*mut/(nA-1)\n",
    "\n",
    "play_counts = g.replicator_dynamics(y0=y0, timepoints=np.linspace(0, 10, 100)\n",
    "                                        ,mutation_matrix=mutation)\n",
    "print(play_counts[-1,:])\n",
    "plt.figure()\n",
    "plt.plot(play_counts)\n",
    "plt.legend(['Defect','Cooperate'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRqSfyXA_onE"
   },
   "source": [
    "In the following cell we compute explicitly the dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGdXwPngVNar"
   },
   "outputs": [],
   "source": [
    "g = Games['Prisioner_dilema']\n",
    "g = Games['UnMatching_pennies']\n",
    "g = Games['Rock-Paper-Scissor']\n",
    "\n",
    "th = np.array([0.5, 0.3, 0.2])\n",
    "dt = 0.01\n",
    "TH = []\n",
    "\n",
    "def dynreplicator(g,th):\n",
    "    # todo\n",
    "    va = g.payoff_matrices[0]@th\n",
    "    v = th@g.payoff_matrices[0]@th\n",
    "\n",
    "    dth = th * (va-v)\n",
    "\n",
    "    return dth\n",
    "\n",
    "for ii in np.arange(0,20,dt):\n",
    "\n",
    "    dth = dynreplicator(g,th)\n",
    "    th = np.clip(th + dth * dt, 0, 1)\n",
    "    th /= np.sum(th)\n",
    "    TH += [th]\n",
    "    #print(th,va,v,dt)\n",
    "    #plt.figure()\n",
    "plt.plot(TH)\n",
    "plt.legend(['Rock','Paper','Scissor'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
